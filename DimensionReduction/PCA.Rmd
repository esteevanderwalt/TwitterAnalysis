---
title: "PCA (Principal Component Analysis)"
output: 
  html_document: 
    keep_md: yes
---

```{r echo=FALSE, messages=FALSE, results='hide'}
#garbage collection + clear RAM
rm(list = ls(all.name = TRUE))
gc()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r include=FALSE}
  #load libraries
  library(knitr)
  library(ggplot2)
  library(scales)
```

One of the most commonly faced problems while dealing with data analytics problem such as recommendation engines, text analytics is high-dimensional and sparse data. At many times, we face a situation where we have a large set of features and fewer data points, or we have data with very high feature vectors. In such scenarios, fitting a model to the dataset, results in lower predictive power of the model. This scenario is often termed as the curse of dimensionality. In general, adding more data points or decreasing the feature space, also known as dimensionality reduction, often reduces the effects of the curse of dimensionality.

Mathematically spoken, PCA is a linear orthogonal transformation that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

Assume you have {n} observations of {p} different variables. Define {X} to be a {(n \times p)} matrix where the {i}-th column of {X} contains the observations of the {i}-th variable, {i = 1, ..., p}. Each row {x_i} of {X} can be represented as a point in a {p}-dimensional space. Therefore, {X} contains {n} points in a {p}-dimensional space.

PCA projects {p}-dimensional data into a {q}-dimensional sub-space {(q \leq p)} in a way that minimizes the residual sum of squares (RSS) of the projection. That is, it minimizes the sum of squared distances from the points to their projections. It turns out that this is equivalent to maximizing the covariance matrix (both in trace and determinant) of the projected data

In probability theory and statistics, covariance is a measure of how much two random variables change together. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive.

## Connect to the database first
```{r echo=FALSE}
read_chunk('../DBConnection/ConnectPostgres.R')
```

```{r connectDB}
```

##Get the tweets
```{r gettwitterfeeddata, cache=FALSE}
users <- dbGetQuery(con, "SELECT * from main.experiment_user_shortest")
```

Total amount of users in the corpus: `r users.total <- nrow(users)
  users.total`

```{r preview_users}
#Preview user dataset  
head(users)

#get numeric dataset
df <- (users[,3:10])
#for later use in biplot
users.names <- (users[,2])

```

##Find the most variance

```{r find_var}
#Use apply() to the corpus row wise to calculate the variance to see how each variable is varying.
apply(df,2,var)
```

Let’s plot all the principal components and see how the variance is accounted with each component.

The resultant components of pca object from the above code corresponds to the standard deviations and Rotation. From the above standard deviations we can observe that the 1st PCA explained most of the variation, followed by other pcas’.  Rotation contains the principal component loadings matrix values which explains /proportion of each variable along each principal component.

```{r pca}
pca =prcomp(df, center = TRUE, scale = TRUE)  #scale and center the values

pca

par(mar = rep(2, 4))
plot(pca)
```

Decide how many PCs to retain for future analysis
```{r pca_decide}
plot(pca, type = "l")
```

The summary method describe the importance of the PCs. The first row describe again the standard deviation associated with each PC. The second row shows the proportion of the variance in the data explained by each component while the third row describe the cumulative proportion of explained variance. 
```{r pca_summary}
summary(pca)
```

Clearly the first principal component accounts for maximum information.
Let us interpret the results of pca using biplot graph. Biplot is used to show the proportions of each variable along the two principal components.

```{r pca_biplot}
#pca$rotation=-pca$rotation
#pca$x=-pca$x
biplot (pca , scale =0)
```

In the preceding image, known as a biplot, we can see the two principal components (PC1 and PC2) of the dataset. The red arrows represent the loading vectors, which represent how the feature space varies along the principal component vectors.

Here is a better version of the plot

```{r pca_biplot_2}
library(devtools)
#install_github("ggbiplot", "vqv")
library(ggbiplot)

#length(pca) #5
#length(users.names) #6846

g <- ggbiplot(pca, obs.scale = 1, var.scale = 1, 
              ellipse = TRUE,  
              circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)
#groups = users.names,  #add this after ellipse if you want to circle groups by another variable
```

##Determine what to keep
Another way of deciding how many components to retain is to use Kaiser’s criterion: that we should only retain principal components for which the variance is above 1 (when principal component analysis was applied to standardised data). We can check this by finding the variance of each of the principal components.
```{r pca_kaiser}
(pca$sdev)^2

#show values to use going forward for regression
loadings <- eigen(cov(df))$vectors
explvar <- loadings^2
explvar
```

```{r closeDB}
```