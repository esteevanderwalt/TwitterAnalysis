---
title: "Experiment results - As Is"
output: 
  html_document: 
    keep_md: yes
---

```{r echo=FALSE, messages=FALSE, results='hide'}
#garbage collection + clear RAM
rm(list = ls(all.name = TRUE))
gc()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r include=FALSE}
  #load libraries
  suppressMessages(library(knitr))
  suppressMessages(library(ggplot2))
  suppressMessages(library(scales))
  suppressMessages(library(caret))
  suppressMessages(library(lubridate))
  suppressMessages(library(doParallel))
  suppressMessages(library(pROC))
  suppressMessages(library(dplyr))
  
```

```{r echo=FALSE}
# Connect to the database first
read_chunk('../../DBConnection/ConnectPostgres.R')
```

```{r connectDB, echo=FALSE}
```

```{r gettwitterfeeddata, cache=FALSE, echo=FALSE}
users.all <- dbGetQuery(con, "SELECT * from main.zz_full_set") 
myvars <- c("name", "screenname", "location", "language", "timezone", "utc_offset",
            "geo_enabled", "latitude", "longitude", "profile_image", 
            "is_default_profile", "is_default_profile_image", "created", "class")
users1.data <- users.all[myvars]
rm(myvars)
```

##Pre-processing
```{r preprocessing}
suppressMessages(library(reshape2))

ggplot_missing <- function(x){
  
  x %>% 
    is.na %>%
    melt %>%
    ggplot(data = .,
           aes(x = Var2,
               y = Var1)) +
    geom_raster(aes(fill = value)) +
    scale_fill_grey(name = "",
                    labels = c("Present","Missing")) +
    theme_minimal() + 
    theme(axis.text.x  = element_text(angle=45, vjust=0.5)) + 
    labs(x = "Variables in Dataset",
         y = "Rows / observations")
}
ggplot_missing(users1.data)

#PREPROCESSING OF DATA
#first replace NA with other
users1.data$timezone[is.na(users1.data$timezone)] <- 'Other'
users1.data$latitude[is.na(users1.data$latitude)] <- 0
users1.data$longitude[is.na(users1.data$longitude)] <- 0

#change created to be year of creation
users1.data$created <- year(ymd_hms(users1.data$created))
users1.data$created[is.na(users1.data$created)] <- 2000

#change location, language, timezone to only have top50 and other
d <- users1.data %>% 
  group_by(location) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
l <- subset(users1.data, !(location %in% d$location[1:50]))$location
users1.data$location[users1.data$location %in% l] <- 'Other'
rm(d, l)

d <- users1.data %>% 
  group_by(timezone) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
l <- subset(users1.data, !(timezone %in% d$timezone[1:20]))$timezone
users1.data$timezone[users1.data$timezone %in% l] <- 'Other'
rm(d, l)

d <- users1.data %>% 
  group_by(language) %>%
  summarise(n=n()) %>%
  arrange(desc(n))
l <- subset(users1.data, !(language %in% d$language[1:20]))$language
users1.data$language[users1.data$language %in% l] <- 'Other'
rm(d, l)

#remove decimals from lat/lon
users1.data$latitude <- round(users1.data$latitude)
users1.data$longitude <- round(users1.data$longitude)

ggplot_missing(users1.data)

#select only those attributes worthy of ML
myvars <- c("utc_offset",
            "geo_enabled", "latitude", "longitude",  
            "is_default_profile", "is_default_profile_image", "created", "class")
ml1.full <- users1.data[myvars]
rm(myvars)
#identify nonzero attributes that can influence result
nzv <- nearZeroVar(ml1.full, saveMetrics= TRUE)
nzv[nzv$nzv,]
rm(nzv)

#create dummy vars
dmy <- dummyVars("class ~ .", data = ml1.full, fullRank=T)
ml1 <- data.frame(predict(dmy, newdata = ml1.full))
rm(dmy)

#identify correlated predictors
descrCor <-  cor(ml1)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)
highCorr
rm(descrCor, highCorr)

```

```{r train_control}
set.seed(123)
inTrain <- createDataPartition(y = ml1.full$class, p = .75, list = FALSE)
#str(inTrain)

training <- ml1[ inTrain,]
testing <- ml1[-inTrain,]
training.class <- ml1.full[ inTrain,]$class
testing.class <- ml1.full[ -inTrain,]$class
#nrow(training)
#nrow(testing)
rm(inTrain)

ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 3,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
```

##PLS - Partial Lease Squares
```{r pls, error=TRUE}
#cl <- makeCluster(detectCores())
#registerDoParallel(cl)
#plsFit <- train(x = training, y = training.class,
#                method = "pls", 
#                tuneLength = 15, 
#                trControl = ctrl, 
#                metric = "ROC",
#                preProc = c("center", "scale"))
#save(plsFit,file="plsFit.RData")

load("plsFit.RData")
#stopCluster(cl)
#registerDoSEQ()

#show result
plsFit
plot(plsFit)
#plot(plsFit, metric="Kappa")
ggplot(plsFit)

plsImportance <- varImp(plsFit, scale=FALSE)
plot(plsImportance)

#predict new values
plsClasses <- predict(plsFit, newdata = testing)
head(plsClasses)

plsProbs <- predict(plsFit, newdata = testing, type = "prob")
head(plsProbs)

confusionMatrix(data = plsClasses, testing.class)

#ROC
#you could look at all predictions over all partitions and resamples at once:
#head(plsFit$pred)
#plot(roc(predictor = plsFit$pred$class, response = plsFit$pred$obs))
#or for each sample
#library(plyr)
#l_ply(split(plsFit$pred, plsFit$pred$Resample), function(d) {
#  plot(roc(predictor = d$class, response = d$obs))
#})

#library(ggplot2)
#library(plotROC)
#selectedIndices <- rfFit$pred$mtry == 2
#ggplot(plsFit$pred[selectedIndices, ], 
#       aes(m = class, d = factor(obs, levels = c("trustworthy","deceptive")))) + 
#  geom_roc(hjust = -0.4, vjust = 1.5) + coord_equal()
plsRoc <- roc(testing.class,plsProbs[,"deceptive"], levels = c("trustworthy","deceptive"))
plsRoc
plot(plsRoc, print.thres="best", print.thres.best.method="closest.topleft")
plsRocCoords <- coords(  plsRoc, "best", best.method="closest.topleft",
                                 ret=c("threshold", "accuracy"))
plsRocCoords

rm(plsRoc, plsRocCoords, plsProbs, plsClasses, plsImportance)

```

##RDA - Regularized Discriminant Analysis
```{r rda, error=TRUE}
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
set.seed(123)

#rdaFit <- train(x = training, y = training.class,
#                method = "rda",
#                tuneGrid = rdaGrid,
#                trControl = ctrl,
#                metric = "ROC")
#save(rdaFit,file="rdaFit.RData")

load("rdaFit.RData")
rdaFit
plot(rdaFit)

rdaImportance <- varImp(rdaFit, scale=FALSE)
plot(rdaImportance)

rdaClasses <- predict(rdaFit, newdata = testing)
rdaProbs <- predict(rdaFit, newdata = testing, type = "prob")
confusionMatrix(rdaClasses, testing.class)

rdaRoc <- roc(testing.class,rdaProbs[,"deceptive"], levels = c("trustworthy","deceptive"))
rdaRoc
plot(rdaRoc, print.thres="best", print.thres.best.method="closest.topleft")
rdaRocCoords <- coords(  rdaRoc, "best", best.method="closest.topleft",
                         ret=c("threshold", "accuracy"))
rdaRocCoords

rm(rdaRoc, rdaRocCoords, rdaProbs, rdaClasses, rdaImportance)
```

###Linear SVM
```{r linearsvm, error=TRUE}
set.seed(123)

svmFit <- train(x = training, y = training.class,
                method = "svmLinear",
                #weights = weights,
                maximize = T,
                tuneGrid = expand.grid(.C=3^(-15:15)),
                trControl = ctrl,
                metric = "ROC")
save(svmFit,file="svmFit.RData")

#load("svmFit.RData")
svmFit
plot(svmFit)

svmImportance <- varImp(svmFit, scale=FALSE)
plot(svmImportance)

svmClasses <- predict(svmFit, newdata = testing)
head(svmClasses)
svmProbs <- predict(svmFit, newdata = testing, type = "prob")
head(svmProbs)
confusionMatrix(svmClasses, testing.class)

svmRoc <- roc(testing.class,svmProbs[,"deceptive"], levels = c("trustworthy","deceptive"))
svmRoc
plot(svmRoc, print.thres="best", print.thres.best.method="closest.topleft")
svmRocCoords <- coords(  svmRoc, "best", best.method="closest.topleft",
                         ret=c("threshold", "accuracy"))
svmRocCoords

rm(svmRoc, svmRocCoords, svmProbs, svmClasses, svmImportance)
```

##RF - Random Forest
```{r rf, error=TRUE}
set.seed(123)
rfFit <- train(x = training, y = training.class,
               method = "rf",
               trControl = ctrl,
               metric = "ROC",
               preProcess = c("center","scale"), 
               tuneLength = 20)
save(rfFit,file="rfFit.RData")

#load("rfFit.RData")
rfFit
plot(rfFit)

# list the chosen features
predictors(rfFit)
# plot the results
plot(rfFit, type=c("g", "o"))

rfImportance <- varImp(rfFit, scale=FALSE)
plot(rfImportance)

rfClasses <- predict(rfFit, newdata = testing)
head(rfClasses)
rfProbs <- predict(rfFit, newdata = testing, type = "prob")
head(rfProbs)
confusionMatrix(rfClasses, testing.class)

rfRoc <- roc(testing.class,rfProbs[,"deceptive"], levels = c("trustworthy","deceptive"))
rfRoc
plot(rfRoc, print.thres="best", print.thres.best.method="closest.topleft")
rfRocCoords <- coords(  rfRoc, "best", best.method="closest.topleft",
                         ret=c("threshold", "accuracy"))
rfRocCoords

rm(rfRoc, rfRocCoords, rfProbs, rfClasses, rfImportance)
```

##KNN - K-Nearest Neighbour
```{r knn, error=TRUE}
set.seed(123)
knnFit <- train(x = training, y = training.class,
               method = "kknn",
               trControl = ctrl,
               metric = "ROC",
               preProcess = c("center","scale"), 
               tuneLength = 20)
save(knnFit,file="knnFit.RData")

#load("knnFit.RData")
knnFit
plot(knnFit)

knnImportance <- varImp(knnFit, scale=FALSE)
plot(knnImportance)

knnClasses <- predict(knnFit, newdata = testing)
head(knnClasses)
knnProbs <- predict(knnFit, newdata = testing, type = "prob")
head(knnProbs)
confusionMatrix(knnClasses, testing.class)

knnRoc <- roc(testing.class,knnProbs[,"deceptive"], levels = c("trustworthy","deceptive"))
knnRoc
plot(knnRoc, print.thres="best", print.thres.best.method="closest.topleft")
knnRocCoords <- coords(  knnRoc, "best", best.method="closest.topleft",
                        ret=c("threshold", "accuracy"))
knnRocCoords

rm(knnRoc, knnRocCoords, knnProbs, knnClasses, knnImportance)
```

##C5.0 - Boosting
```{r c50, error=TRUE}
set.seed(123)
c50fit <- train(x=training, y=training.class, 
                method = "C5.0", 
                metric = "ROC", 
                trControl=control,
                verbose=FALSE)
save(c50fit,file="c50fit.RData")

#load("c50Fit.RData")
c50fit
# visualize the resample distributions
xyplot(c50fit,type = c("g", "p", "smooth"))

plot(c50fit)

c50Importance <- varImp(c50fit, scale=FALSE)
plot(c50Importance)

c50Classes <- predict(c50fit, newdata = testing)
head(c50Classes)
c50Probs <- predict(c50fit, newdata = testing, type = "prob")
head(c50Probs)
confusionMatrix(c50Classes, testing.class)

c50Roc <- roc(testing.class,c50Probs[,"deceptive"], levels = c("trustworthy","deceptive"))
c50Roc
plot(c50Roc, print.thres="best", print.thres.best.method="closest.topleft")
c50RocCoords <- coords(  c50Roc, "best", best.method="closest.topleft",
                         ret=c("threshold", "accuracy"))
c50RocCoords

rm(c50Roc, c50RocCoords, c50Probs, c50Classes, c50Importance)
   
```

###################
##Final comparison
###################
```{r compare, error=TRUE}
#How do these models compare in terms of their resampling results? The resamples function can be
#used to collect, summarize and contrast the resampling results. Since the random number seeds
#were initialized to the same value prior to calling train, the same folds were used for each model.
resamps <- resamples(list(pls = plsFit, rda = rdaFit, svm = svmFit, rf = rfFit, knn = knnFit, c50 = c50fit))
summary(resamps)
xyplot(resamps, what = "BlandAltman") 

#Since, for each resample, there are paired results a paired t–test can be used to assess whether there
#is a difference in the average resampled area under the ROC curve. The diff.resamples function
#can be used to compute this:
diffs <- diff(resamps)
summary(diffs)
```