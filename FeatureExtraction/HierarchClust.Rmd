---
title: "Hierarchical clustering"
output: 
  html_document: 
    keep_md: yes
---

```{r echo=FALSE, messages=FALSE, results='hide'}
#garbage collection + clear RAM
rm(list = ls(all.name = TRUE))
gc()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r include=FALSE}
  #load libraries
  library(knitr)
  library(ggplot2)
  library(scales)
```

K means clustering requires us to specify the number of clusters, and finding the optimal number of clusters can often be hard. Hierarchical clustering is an alternative approach which builds a hierarchy from the bottom-up, and doesnâ€™t require us to specify the number of clusters beforehand.

The algorithm works as follows:

- Put each data point in its own cluster.
- Identify the closest two clusters and combine them into one cluster.
- Repeat the above step till all the data points are in a single cluster.
- Once this is done, it is usually represented by a dendrogram like structure.

There are a few ways to determine how close two clusters are:

- Complete linkage clustering: Find the maximum possible distance between points belonging to two different clusters.
- Single linkage clustering: Find the minimum possible distance between points belonging to two different clusters.
- Mean linkage clustering: Find all possible pairwise distances for points belonging to two different clusters and then calculate the average.
- Centroid linkage clustering: Find the centroid of each cluster and calculate the distance between centroids of two clusters.

Complete linkage and mean linkage clustering are the ones used most often.

## Connect to the database first
```{r echo=FALSE}
read_chunk('../DBConnection/ConnectPostgres.R')
```

```{r connectDB}
```

##Get the tweets
```{r gettwitterfeeddata, cache=FALSE}
users <- dbGetQuery(con, "SELECT * from main.experiment_user_shortest")
```

Total amount of users in the corpus: `r users.total <- nrow(users)
  users.total`

```{r preview_users}
#Preview user dataset  
head(users)

#scale the dataset
df <- scale(users[,3:10])

#Preview scaled dataset
head(df)
```

##Train the data
We need to find the optimal amount of clusters first

###Lets look at: no_of_tweets vs no_of_replies:

```{r cluster_size_1}
mydata <- df[, 1:2]

set.seed(20)
clusters <- hclust(dist(mydata)) #default method = complete linkage
plot(clusters)

clusters <- hclust(dist(mydata), method = 'average')
plot(clusters)
groups <- cutree(clusters, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(clusters, k=5, border="red")
```



```{r closeDB}
```