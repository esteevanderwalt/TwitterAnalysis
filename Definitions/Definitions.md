# Tweet Corpus Information



## Definitions used

### PCA
PCA creates a low-dimensional representation of the samples from a data set which is optimal in the sense that it contains as much of the variance in the original data set as is possible.

### PCA vs Kmeans

The intuition is that PCA seeks to represent all n data vectors as linear combinations of a small number of eigenvectors, and does it to minimize the mean-squared reconstruction error. In contrast, K-means seeks to represent all n data vectors via small number of cluster centroids, i.e. to represent them as linear combinations of a small number of cluster centroid vectors where linear combination weights must be all zero except for the single 11. This is also done to minimize the mean-squared reconstruction error.

### Factor analysis (FA)

It groups similar variables into dimensions. If variables correlate highly, they might measure aspects of a common underlying dimension.These dimensions are called factors.
Mainly used for data reduction such reducing the length of the questionnaire.
E.g. Attends loud parties; talks a lot; appears comfortable interacting with anyone; is usually seen with others– all of these variables could be measuring the central concept or factor of “outgoing”

### PCA vs FA
The key interpretation difference between PCA and FA is that the latter constructs latent variables. Without going into detail, I have seen in the past some arguments that FA should be used for social science (where there are few direct measures of the variables of interest, so proxy variables are used, and therefore one is interested in the latent, unmeasured variables) and PCA better fits with the more physical sciences (where there are direct measures such as distance, weight, volume). 

### Cluster analysis
It is technique that used to classify objects or cases into relatively homogeneous groups called clusters.
E.g. “Young couples, young families, older families, pre-retirement and retired” are clusters based on demographic variables.

### Eigenvectors
In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that does not change its direction when that linear transformation is applied to it.
 λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.

### Multivariate analysis
Multivariate analysis (MVA) is based on the statistical principle of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time.

### Discrete variables
Variables that can only take on a finite number of values are called "discrete variables." All qualitative variables are discrete. Some quantitative variables are discrete, such as performance rated as 1,2,3,4, or 5, or temperature rounded to the nearest degree.
