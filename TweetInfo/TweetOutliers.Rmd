---
title: "Tweet Outliers"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
```

```{r include=FALSE}
  #load libraries
  library(knitr)
  library(ggplot2)
  library(lubridate)
  library(scales)
```

In statistics, a outlier is defined as a observation which stands far away from the most of other observations.
There are different methods to detect the outliers, including standard deviation approach and Tukey’s method which use interquartile (IQR) range approach.

Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately poorer results.
Even before predictive models are prepared on training data, outliers can result in misleading representations and in turn misleading interpretations of collected data. Outliers can skew the summary distribution of attribute values in descriptive statistics like mean and standard deviation and in plots such as histograms and scatterplots, compressing the body of the data.
Finally, outliers can represent examples of data instances that are relevant to the problem such as anomalies in the case of fraud detection and computer security.

Taxonomy of outlier detection methods, as follows:
•Extreme Value Analysis: Determine the statistical tails of the underlying distribution of the data. For example, statistical methods like the z-scores on univariate data.
•Probabilistic and Statistical Models: Determine unlikely instances from a probabilistic model of the data. For example, gaussian mixture models optimized using expectation-maximization.
•Linear Models: Projection methods that model the data into lower dimensions using linear correlations. For example, principle component analysis and data with large residual errors may be outliers.
•Proximity-based Models: Data instances that are isolated from the mass of the data as determined by cluster, density or nearest neighbor analysis.
•Information Theoretic Models: Outliers are detected as data instances that increase the complexity (minimum code length) of the dataset.
•High-Dimensional Outlier Detection: Methods that search subspaces for outliers give the breakdown of distance based measures in higher dimensions (curse of dimensionality).

Outliers are interesting. Depending on the context, they either deserve special attention or should be completely ignored. Take the example of revenue forecasting. If unusual spikes of revenue are observed, it's probably a good idea to pay extra attention to them and figure out what caused the spike. But if the outliers are due to mechanical error, measurement error or anything else that’s not generalizable, it’s a good idea to filter out these outliers before feeding the data to the modeling algorithm. 
Some models are more sensitive to outliers than others. For instance, AdaBoost might treat those outliers as "hard" cases and put tremendous weights on outliers while decision tree might simply count each outlier as one false classification. If the data set contains a fair amount of outliers, it's important to either use modeling algorithm robust against outliers or filter the outliers out. 

The aim is to understand which of the variables in twitter contain outliers, if at all.
This will help us determine if we need to clean the corpus from outliers before any machine learning.

## Connect to the database first
```{r echo=FALSE}
read_chunk('../DBConnection/ConnectPostgres.R')
```

```{r connectDB}
```

Connection success: `r dbExistsTable(con, c("main","experiment_tweets_shortest"))`

##Pull data from the database
We will pull the full corpus
##Get the tweets
```{r gettwitterfeeddata, cache=FALSE}
users <- dbGetQuery(con, "SELECT * from main.experiment_user_shortest")
```

Total amount of users in the corpus: `r users.total <- nrow(users)
  users.total`

##number of variables
```{r variables}
#number of variables
ncol(users)

#structure
  str(users)
```

##Box plots of each tweet variable
```{r boxplot}
#first remove all non numeric columns
#get columns that has numeric values
nums <- sapply(users, is.numeric)
#Then standard subsetting
users <- users[ , nums]
str(users)

#draw boxplot for each variable with outliers
boxplot(users, horizontal=TRUE,las=2)

#draw boxplot for each variable without outliers
boxplot(users, horizontal=TRUE, outline=FALSE, las=2)

#userid is unique therefore remove from set as it wont serve any purpose in machine learning algorithm
users <- users[ , 2:ncol(users)]
#draw boxplot for each variable with outliers seperately
for (i in 1:ncol(users)) {
  x <- users[,i,drop=FALSE]
  n <- colnames(users[i])
  boxplot(x,horizontal=TRUE,outline=TRUE,ylab=n)
  hist(users[,i], main="", xlab = n) #only accepts vector
}


#for (i in 1:ncol(users)) 
#  boxplot(users[,i,drop=FALSE],horizontal=TRUE,axes=FALSE,outline=FALSE)
```

## show data in terms of categories related to each other
###users.text
This category is not important for now. We are first investigating the user themselves before we will look at what they wrote
users.text

###users.numbers
```{r user_numbers}
users.numbers <- users[,1:7]
n <- colnames(users.numbers)
boxplot(users.numbers,horizontal=TRUE,outline=TRUE,names=n, las=2)
```

###users.images
```{r user_images}
users.images <- users[,11:ncol(users)]
n <- colnames(users.images)
boxplot(users.images,horizontal=TRUE,outline=TRUE,names=n, las=2)
```

###users.gps
```{r user_gps}
users.gps <- users[,8:10]
n <- colnames(users.gps)
boxplot(users.gps,horizontal=TRUE,outline=TRUE,names=n, las=2)
```

##Close the Database connection
```{r closeDB}
```